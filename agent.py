import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
import numpy as np
from collections import defaultdict
import time
#This is the primary entity that we are dealing with here. It is the "agent" or the car which is going to act based on the input and feedback that it gets 
#from the environment. 
#Broadly, the sequence of actions that it going through are -->
#
#
#

class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """ 

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment 
        self.planner = RoutePlanner(self.env, self)  # Create a route planner. 
        #TODO. Understand how the routeplanner really gets created / operated in this environment. 

        self.valid_actions = self.env.valid_actions  # The set of valid actions [right now, they are left, right, forward, none]

        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn. [true / false. need conditionality all across the code basis value of this]
        #self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
        self.Q  = defaultdict(dict) #IMPORTANT : Problem with just creating a dictionary, is that assignment of 2 dimentional tupes arent as straightforward. This apparently simplifies that
        self.epsilon = epsilon   # Random exploration factor. You start with an "Exploration" factor, and keep tighening it basis a decay
        #E.g. if the exploration is 1, 100% of actions would be random. but on every round, this reduces byt he decay, which would reduce the randomization, and exploration as well
        self.alpha = alpha       # Learning factor

        self.trial_num = 0
        self.num_times_in_learn = 0
        self.arrtest = []
        ###########
        ## TO DO ##
        ###########
        # Set any additional class parameters as needed


    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)
        
        ########### 
        ## TO DO ##
        ###########
        # Update epsilon using a decay function of your choice
        # Update additional class parameters as needed
        # If 'testing' is True, set epsilon and alpha to 0
        # since in this case, all actions are random, and there is no "learning" or "Testing"

        
        if testing == True:
            self.epsilon = 0.0
            self.alpha = 0.0
        else:
            #self.alpha = 0.5
            #self.alpha = self.alpha * 0.95
            #self.epsilon = self.epsilon - 0.05*self.epsilon
            
            self.epsilon = self.epsilon - 0.005
            self.alpha = self.epsilon / 2
            self.trial_num = self.trial_num + 1
            
            print str(self.trial_num) + " : this is trial number"
            print "%.2f" % round(self.epsilon,2) 

            
        return None

    def build_state(self):
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint() # The next waypoint 
        inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline

        ########### 
        ## TO DO ##
        ###########
        print inputs
        print type(waypoint)
        print type(inputs)
        print str(inputs)
        # Set 'state' as a tuple of relevant data for the agent        
        state = (waypoint, str(inputs)) 
        #exit()
        #, removing the deadline from the state to avoid it from getting to complicated, and requiring the amount of data to be exponentially higher
        return state


    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """
        if self.learning == False:
            return 0
        ########### 
        ## TO DO ##
        ###########
        # Calculate the maximum Q-value of all actions for a given state

        maxQ = max(self.Q[state].values())
        return maxQ 


    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        ########### 
        ## TO DO ##
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0
        #print state
        if self.learning == True:

            print "in createQ"
            print self.Q
            if state not in self.Q:
                print "adding values in the dictionary"
                self.Q[state]['left'] = 0
                self.Q[state]['right'] = 0
                self.Q[state]['forward'] = 0
                self.Q[state][None] = 0

        return


    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """
        # Set the agent state and default action
        print state
        print "in choose action"
        #exit()
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()
        r = random.randint(1, 100)
        print r
        print self.epsilon*100
        #here is the greedy exploration function. Setting up a functio, so that at E probability, we choose a random action. And keep reducing that as we go ahead. 
        if r < self.epsilon*100:
            #random number
            action = random.choice(self.env.valid_actions)
            #action = self.next_waypoint
        else:
            print state
            print self.Q
            maxqvalue = self.get_maxQ(state)
            possact = []

            for k,v in self.Q[state].items():

                if maxqvalue == v:
                    possact.append(k)
            #print possact
            action = random.choice(possact)
            #action = self.next_waypoint
        print "action is " + str(action)
        #
        ########### 
        ## TO DO ##
        ###########
        # When not learning, choose a random action
        # When learning, choose a random action with 'epsilon' probability
        #   Otherwise, choose an action with the highest Q-value for the current state
 
        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives an award. This function does not consider future rewards 
            when conducting learning. """
        ########### 
        ## TO DO ##
        ###########
        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')
        if self.learning == False:
            return
        self.num_times_in_learn = self.num_times_in_learn + 1
        if state not in self.arrtest:
            self.arrtest.append(state)
        print "in learn"
        print state
        print type(state)
        #exit()
        if state in self.Q:
            if action in self.Q[state]:
                print "in the loop"
                print self.Q
                self.Q[state][action] = reward*(self.alpha) + self.Q[state][action] * (1-self.alpha)
            else:
                self.Q[state][action] = reward*(self.alpha)
       
        return


    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """

        state = self.build_state()          # Get current state
        print "before create"
        #print self.Q[state]
        self.createQ(state)                 # Create 'state' in Q-table
        print "After create"
        print self.Q[state]
        action = self.choose_action(state)  # Choose an action
        print action
        print type(action)
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn

        return
        

def run():
    """ Driving function for running the simulation. 
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """

    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment(verbose=True, num_dummies=100)
    
    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    agent = env.create_agent(LearningAgent, learning=True, epsilon=1, alpha=.5)
    
    ##############
    # Follow the driving agent
    # Flags:
    env.set_primary_agent(agent, enforce_deadline=True)

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics = True #  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env, update_delay=0.01, log_metrics=True, optimized=True)
    
    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(n_test=10, tolerance=0.01)
    print len(agent.Q)
    f = open("dump.txt", "w+")
    f.write(agent.Q)
    f.close()



if __name__ == '__main__':
    run()
